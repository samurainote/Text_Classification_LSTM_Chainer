{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Learning for NLP challenge\n",
    "# 【文書分類】Text Classification using LSTM with Chainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](https://miro.medium.com/max/960/1*HgXA9v1EsqlrRDaC_iORhQ.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ---------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 0. Task Definition\n",
    "\n",
    "### Goal  \n",
    "文章をカテゴリごとに正確に分類するAIを作る   \n",
    "### Business Impact  \n",
    "メディアドメインでインパクト大       \n",
    "### Algorithm  \n",
    "CNN, RNN, LSTM   \n",
    "### Framework  \n",
    "Chainer  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](https://www.preferred-networks.jp/wp-content/uploads/2017/02/chainer_red_h.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chainerの基本ファンクションについて\n",
    "  \n",
    "### Chain  \n",
    "複数のLinkの組み合わせ（ニューラルネットワーク）をChainの配下にLinkを作ってまとめ上げることで、一つのChainオブジェクトとして管理する   \n",
    "### Links    \n",
    "y = x1w1 + x2w2 + bなどの学習可能なパラメータである「重み w」と「バイアス b」などの学習可能なパラメータを持った最適化のための関数   \n",
    "### Function  \n",
    "Linksが学習したパラメータ（Wとかｂとか）を持つ関数ならFunctionsはパラメータを持たない関数   \n",
    "### Variable  \n",
    "変数   \n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ---------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1. Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "101"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = [\n",
    "    [\"Could I exchange business cards, if you don’t mind?\", 1],\n",
    "    [\"I'm calling regarding the position advertised in the newspaper.\", 0],\n",
    "    [\"I'd like to apply for the programmer position.\", 0],\n",
    "    [\"Could you tell me what an applicant needs to submit?\", 1],\n",
    "    [\"Could you tell me what skills are required?\", 1],\n",
    "    [\"We will assist employees with training and skill development.\", 0],\n",
    "    [\"What kind of in-house training system do you have for your new recruits?\", 1],\n",
    "    [\"For office equipment I think rental is better.\", 0],\n",
    "    [\"Is promotion based on the seniority system?\", 1],\n",
    "    [\"What's still pending from February?\", 1],\n",
    "    [\"Which is better, rental or outright purchase?\", 1],\n",
    "    [\"General Administration should do all the preparations for stockholder meetings.\", 0],\n",
    "    [\"One of the elevators is out of order. When do you think you can have it fixed?\", 1],\n",
    "    [\"General Administration is in charge of office building maintenance.\", 0],\n",
    "    [\"Receptionists at the entrance hall belong to General Administration.\", 0],\n",
    "    [\"Who is managing the office supplies inventory?\", 1],\n",
    "    [\"Is there any difference in pay between males and females?\", 1],\n",
    "    [\"The General Administration Dept. is in charge of office maintenance.\", 0],\n",
    "    [\"Have you issued the meeting notice to shareholders?\", 1],\n",
    "    [\"What is an average annual income in Japan?\", 1],\n",
    "    [\"Many Japanese companies introduced the early retirement system.\", 0],\n",
    "    [\"How much did you pay for the office equipment?\", 1],\n",
    "    [\"Is the employee training very popular here?\", 1],\n",
    "    [\"What kind of amount do you have in mind?\", 1],\n",
    "    [\"We must prepare our financial statement by next Monday.\", 0],\n",
    "    [\"Would it be possible if we check the draft?\", 1],\n",
    "    [\"The depreciation of fixed assets amounts to $5 million this year.\", 0],\n",
    "    [\"Please expedite the completion of the balance sheet.\", 0],\n",
    "    [\"Could you increase the maximum lending limit for us?\", 1],\n",
    "    [\"We should cut down on unnecessary expenses to improve our profit ratio.\", 0],\n",
    "    [\"What percentage of revenue are we spending for ads?\", 1],\n",
    "    [\"One of the objectives of internal auditing is to improve business efficiency.\", 0],\n",
    "    [\"Did you have any problems finding us?\", 1],\n",
    "    [\"How is your business going?\", 1],\n",
    "    [\"Not really well. I might just sell the business.\", 0],\n",
    "    [\"What line of business are you in?\", 1],\n",
    "    [\"He has been a valued client of our bank for many years.\", 0],\n",
    "    [\"Would you like for me to show you around our office?\", 1],\n",
    "    [\"It's the second door on your left down this hall.\", 0],\n",
    "    [\"This is the … I was telling you about earlier.\", 0],\n",
    "    [\"We would like to take you out to dinner tonight.\", 0],\n",
    "    [\"Could you reschedule my appointment for next Wednesday?\", 1],\n",
    "    [\"Would you like Japanese, Chinese, Italian, French or American?\", 1],\n",
    "    [\"Is there anything you prefer not to have?\", 1],\n",
    "    [\"Please give my regards to the staff back in San Francisco.\", 0],\n",
    "    [\"This is a little expression of our thanks.\", 0],\n",
    "    [\"Why don’t you come along with us to the party this evening?\", 1],\n",
    "    [\"Unfortunately, I have a prior engagement on that day.\", 0],\n",
    "    [\"I am very happy to see all of you today.\", 0],\n",
    "    [\"It is a great honor to be given this opportunity to present here.\", 0],\n",
    "    [\"The purpose of this presentation is to show you the new direction our business is taking in 2009.\", 0],\n",
    "    [\"Could you please elaborate on that?\", 1],\n",
    "    [\"What's your proposal?\", 1],\n",
    "    [\"That's exactly the point at issue here.\", 0],\n",
    "    [\"What happens if our goods arrive after the delivery dates?\", 1],\n",
    "    [\"I'm afraid that's not accpetable to us.\", 0],\n",
    "    [\"Does that mean you can deliver the parts within three months?\", 1],\n",
    "    [\"We can deliver parts in as little as 5 to 10 business days.\", 0],\n",
    "    [\"We've considered all the points you've put forward and our final offer is $900.\", 0],\n",
    "    [\"Excuse me but, could I have your name again, please?\", 1],\n",
    "    [\"It's interesting that you'd say that.\", 0],\n",
    "    [\"The pleasure's all ours. Thank you for coimng today.\", 0],\n",
    "    [\"Could you spare me a little of your time？\", 1],\n",
    "    [\"That's more your area of expertise than mine, so I'd like to hear more.\", 0],\n",
    "    [\"I'd like to talk to you about the new project.\", 0],\n",
    "    [\"What time is convenient for you?\", 1],\n",
    "    [\"How’s 3:30 on Tuesday the 25th?\", 1],\n",
    "    [\"Could you inform us of the most convenient dates for our visit?\", 1],\n",
    "    [\"Fortunately, I was able to return to my office in time for the appointment.\", 0],\n",
    "    [\"I am sorry, but we have to postpone our appointment until next month.\", 0],\n",
    "    [\"Great, see you tomorrow then.\", 0],\n",
    "    [\"Great, see you tomorrow then.\", 1],\n",
    "    [\"I would like to call on you sometime in the morning.\", 0],\n",
    "    [\"I'm terribly sorry for being late for the appointment.\", 0],\n",
    "    [\"Could we reschedule it for next week?\", 1],\n",
    "    [\"I have to fly to New York tomorrow, can we reschedule our meeting when I get back?\", 1],\n",
    "    [\"I'm looking forward to seeing you then.\", 0],\n",
    "    [\"Would you mind writing down your name and contact information?\", 1],\n",
    "    [\"I'm sorry for keeping you waiting.\", 0],\n",
    "    [\"Did you find your way to our office wit no problem?\", 1],\n",
    "    [\"I need to discuss this with my superior. I'll get back to you with our answer next week.\", 0],\n",
    "    [\"I'll get back to you with our answer next week.\", 0],\n",
    "    [\"Thank you for your time seeing me.\", 0],\n",
    "    [\"What does your company do?\", 1],\n",
    "    [\"Could I ask you to make three more copies of this?\", 1],\n",
    "    [\"We have appreciated your business.\", 0],\n",
    "    [\"When can I have the contract signed?\", 1],\n",
    "    [\"His secretary is coming down now.\", 0],\n",
    "    [\"Please take the elevator on your right to the 10th floor.\", 0],\n",
    "    [\"Would you like to leave a message?\", 1],\n",
    "    [\"It's downstairs in the basement.\", 0],\n",
    "    [\"Your meeting will be held at the main conference room on the 15th floor of the next building.\", 0],\n",
    "    [\"Actually, it is a bit higher than expected. Could you lower it?\", 1],\n",
    "    [\"We offer the best price anywhere.\", 0],\n",
    "    [\"All products come with a 10-year warranty.\", 0],\n",
    "    [\"It sounds good, however, is made to still think; seem to have a problem.\", 0],\n",
    "    [\"Why do you need to change the unit price?\", 1],\n",
    "    [\"Could you please tell me the gist of the article you are writing?\", 1],\n",
    "    [\"Would you mind sending or faxing your request to me?\", 1],\n",
    "    [\"About when are you publishing this book?\", 1],\n",
    "    [\"May I record the interview?\", 1]\n",
    "]\n",
    "\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ---------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2. Build Neural Network Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](https://cdn-images-1.medium.com/max/2600/1*sO-SP58T4brE9EHazHSeGA.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 分類器をつくるステップ\n",
    "\n",
    "1. ネットワーク形状を決める\n",
    "2. パラメータ（Weight&Bias）に適当な数字をいれる\n",
    "3. あるデータセットを入れてみて、出力を確認する\n",
    "4. 正解データからのずれから、適切にパラメータを調整する   \n",
    "*以下、「学習がそれなりにうまくいった」と思えるまで繰り返し\n",
    "  \n",
    "  \n",
    "具体的には、Chainerは「1. ネットワーク形状を決める」以外のすべてを肩代わりしてくれます。プログラマが行うのは\n",
    "\n",
    "1. ネットワーク形状を決める\n",
    "2. 正解データを用意する\n",
    "3. 最適化手法を選ぶ\n",
    "4. 最適化のためのパラメータ(エポック数やミニバッチサイズ等)を決める  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A) Define Network Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chainer\n",
    "from chainer import Chain\n",
    "import chainer.links as L\n",
    "import chainer.functions as F\n",
    "\n",
    "# モデルクラスの定義\n",
    "class LSTM_TextClassifier(Chain):\n",
    "    \n",
    "    \"\"\"「単語IDを入力して、記事のカテゴリを分類する」ニューラルネットワークを設計する \"\"\"\n",
    "    # vocab_sizeは単語の種類数、vector_sizeは単語ベクトルの次元数、hidden_sizeは隠れ層の次元数、out_sizeは分類するカテゴリ数を表す\n",
    "    def __init__(self, vocab_size, vector_size, hidden_size, out_size):      \n",
    "        \n",
    "        super(LSTM_TextClassifier, self).__init__(\n",
    "            # EmbedIDは入力側がone-hotベクトルの場合のLinearで、ベクトルの代わりに発火している要素のIDを渡すことができる\n",
    "            wv = L.EmbedID(vocab_size, vector_size, ignore_label=1),\n",
    "            # モデルを定義する（LSTM）\n",
    "            vh = L.LSTM(vector_size, hidden_size),\n",
    "            # 重みとバイアスが全て線形作用素（行列）なのでLinearを使う\n",
    "            hh = L.Linear(hidden_size, hidden_size),\n",
    "            hy = L.Linear(hidden_size, out_size)\n",
    "        )\n",
    "    \n",
    "    \"\"\" 実際の計算を行うforward（順伝播）関数を定義する \"\"\"\n",
    "    # 言語モデルの場合は次の式に表す文の結合確率を求めることになる\n",
    "    def __call__(self, x):\n",
    "        \n",
    "        # エンコード\n",
    "        x = F.transpose_sequence(x)\n",
    "        self.vh.reset_state()\n",
    "        for word in x:\n",
    "            e = self.wv(word)\n",
    "            h = self.vh(e)\n",
    "            \n",
    "            # 分類\n",
    "            y = self.hy(h)\n",
    "            return y\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "＊Chainerはミニバッチ処理が前提になっているので、データの次元がひとつ多くなっています（コード中でバッチ処理は行っていない）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### データを「テキスト」と「ラベル」に分割する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = len(data)\n",
    "texts = []\n",
    "labels = []\n",
    "for e in data:\n",
    "    texts.append(e[0])\n",
    "    labels.append(e[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 正規表現関数を作成する：文章の標準化、単語化、記号や数字の削除、ストップワードの除外"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def seq2word(text):\n",
    "    stopwords = [\"i\", \"a\", \"an\", \"the\", \"and\", \"or\", \"if\", \"is\", \"are\", \"am\", \"it\", \"this\", \"that\", \"of\", \"from\", \"in\", \"on\"]\n",
    "    # 小文字化\n",
    "    text = text.lower()\n",
    "    # 改行を削除\n",
    "    text = text.replace(\"\\n\", \"\")\n",
    "    # re.compile()で同じパターンを繰り返し使用する&re.sub()でマッチした部分を置換する\n",
    "    text = re.sub(re.compile(r\"[!-\\/:-@[-`{-~]\"), \" \", text)\n",
    "    # re.split()はパターンにマッチした部分で文字列を分割&リストにして返す\n",
    "    text = text.split(\" \")\n",
    "    \n",
    "    words = []\n",
    "    for word in text:\n",
    "        if ( re.compile(r\"^.*[0-9]+.*$\").fullmatch(word) is not None ):\n",
    "            continue\n",
    "        if word in stopwords:\n",
    "            continue\n",
    "        words.append(word)\n",
    "    return words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 上記の正規表現関数にテキストデータを流して単語辞書を作成する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = {}\n",
    "for text in texts:\n",
    "    words = seq2word(text)\n",
    "    for word in words:\n",
    "        if word not in corpus:\n",
    "            corpus[word] = len(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "単語数 370\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "                    4\n",
       "able              286\n",
       "about             183\n",
       "accpetable        240\n",
       "actually          343\n",
       "administration     60\n",
       "dtype: int64"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "print(\"単語数\", len(corpus))\n",
    "inside_corpus = pd.Series(corpus)\n",
    "inside_corpus.head(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 文章を単語ID配列にする"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_vec = []\n",
    "for text in texts:\n",
    "    words = seq2word(text)\n",
    "    words_id = []\n",
    "    for word in words:\n",
    "        words_id.append(corpus[word])\n",
    "    texts_vec.append(words_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "単語ID配列で表現される文書数： 101\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0              [0, 1, 2, 3, 4, 5, 6, 7, 4]\n",
       "1                [8, 9, 10, 11, 12, 13, 4]\n",
       "2          [14, 15, 16, 17, 18, 19, 11, 4]\n",
       "3    [0, 5, 20, 21, 22, 23, 24, 16, 25, 4]\n",
       "4            [0, 5, 20, 21, 22, 26, 27, 4]\n",
       "5      [28, 29, 30, 31, 32, 33, 34, 35, 4]\n",
       "dtype: object"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"単語ID配列で表現される文書数：\", len(texts_vec))\n",
    "inside_texts_vec = pd.Series(texts_vec)\n",
    "inside_texts_vec.head(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 【標準化】 文章の長さを揃える（前パディングする）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_text_size = 0\n",
    "\n",
    "for text_vec in texts_vec:\n",
    "    if max_text_size < len(text_vec):\n",
    "        max_text_size = len(text_vec)\n",
    "for words_id in texts_vec:\n",
    "    while len(words_id) < max_text_size:\n",
    "        words_id.insert(0, -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 【高速化】計算効率を上げる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "texts_vec = np.array(texts_vec, dtype=\"int32\")\n",
    "labels = np.array(labels, dtype=\"int32\")\n",
    "dataset = []\n",
    "\n",
    "# ベクトル化された文章とラベルをまとめる\n",
    "for v, l in zip(texts_vec, labels):\n",
    "    dataset.append((v, l))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 【正則化】学習率を定義\n",
    "![title](https://cdn-images-1.medium.com/max/800/1*i_lp_hUFyUD_Sq4pLer28g.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### エポック数：「訓練データを一巡したら1カウントされる数」のこと   \n",
    "#### バッチサイズ：「一回の学習に使うデータの個数」ことで、入力データを一定数の束（ミニバッチ）に分割したものです"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCH_NUM = 10\n",
    "BATCH_SIZE = 5\n",
    "EMBED_SIZE = 200\n",
    "HIDDEN_SIZE = 100\n",
    "OUT_SIZE = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 【最適化】：勾配降下法の勾配方法を定義（今回はAdam）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](https://camo.qiitausercontent.com/2d025ad02dd4676ecc34a095954b08b7457c7ddc/687474703a2f2f73656261737469616e72756465722e636f6d2f636f6e74656e742f696d616765732f323031362f30312f736164646c655f706f696e745f6576616c756174696f6e5f6f7074696d697a6572732e676966)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<chainer.optimizers.adam.Adam at 0x11f4f1908>"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from chainer import optimizers\n",
    "\n",
    "model = L.Classifier(LSTM_TextClassifier(\n",
    "    vocab_size=len(corpus),\n",
    "    vector_size=EMBED_SIZE,\n",
    "    hidden_size=HIDDEN_SIZE,\n",
    "    out_size=OUT_SIZE\n",
    "))\n",
    "\n",
    "optimizer = optimizers.Adam()\n",
    "optimizer.setup(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3. Model Evaluation and Tuning Parameters\n",
    "Chainerのアーキテクチャ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](https://camo.qiitausercontent.com/d3af5d369c038fed989ea7839b1566ef49a6c331/68747470733a2f2f71696974612d696d6167652d73746f72652e73332e616d617a6f6e6177732e636f6d2f302f31373933342f61373531646633312d623939392d663639322d643833392d3438386332366231633438612e706e67)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "from chainer import training\n",
    "from chainer.training import extensions\n",
    "\n",
    "train, test = chainer.datasets.split_dataset_random(dataset, N-20)\n",
    "train_iter = chainer.iterators.SerialIterator(train, BATCH_SIZE)\n",
    "test_iter = chainer.iterators.SerialIterator(test, BATCH_SIZE, repeat=False, shuffle=False)\n",
    "updater = training.StandardUpdater(train_iter, optimizer, device=-1)\n",
    "trainer = training.Trainer(updater, (EPOCH_NUM, \"epoch\"), out=\"result\")\n",
    "trainer.extend(extensions.Evaluator(test_iter, model, device=-1))\n",
    "trainer.extend(extensions.LogReport(trigger=(1, \"epoch\")))\n",
    "# エポック数、学習損失、テスト損失、学習正解率、テスト正解率、経過時間を表示する\n",
    "trainer.extend(extensions.PrintReport([\"epoch\", \"main/loss\", \"validation/main/loss\", \"main/accuracy\", \"validation/main/accuracy\", \"elapsed_time\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch       main/loss   validation/main/loss  main/accuracy  validation/main/accuracy  elapsed_time\n",
      "\u001b[J1           0.73926     0.774251              0.411765       0.35                      0.324076      \n",
      "\u001b[J2           0.711038    0.711952              0.5375         0.35                      0.486369      \n",
      "\u001b[J3           0.688513    0.694492              0.45           0.35                      0.635327      \n",
      "\u001b[J4           0.685435    0.728399              0.5375         0.35                      0.781089      \n",
      "\u001b[J5           0.692864    0.713429              0.5375         0.35                      0.924365      \n",
      "\u001b[J6           0.682774    0.745042              0.564706       0.35                      1.09401       \n",
      "\u001b[J7           0.693282    0.702431              0.5375         0.35                      1.24367       \n",
      "\u001b[J8           0.686603    0.716328              0.525          0.35                      1.41009       \n",
      "\u001b[J9           0.682935    0.725888              0.55           0.35                      1.54985       \n",
      "\u001b[J10          0.685435    0.718345              0.5375         0.35                      1.69043       \n"
     ]
    }
   ],
   "source": [
    "trainer.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 【結論】\n",
    "35%しか正解率がない\n",
    "要調整が必要"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
